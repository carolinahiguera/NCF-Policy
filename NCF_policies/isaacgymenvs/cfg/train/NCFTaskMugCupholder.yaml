seed: ${..seed}
algo: PPO
network:
  mlp:
    units: [256, 128, 64]
    # units: [64, 64]
  tactile_mlp:
    units: [256, 64]
  ncf_mlp:
    units: [256, 128]
  ncf_adapt_mlp:
    units: [256, 128, 128]
    # units: [128]

load_path: ${..checkpoint} # path to the checkpoint to load

ppo:
  output_name: 'tactile_cupholder_1'
  multi_gpu: False
  normalize_input: True
  normalize_value: True
  normalize_point_cloud: False
  value_bootstrap: True
  num_actors: ${...task.env.numEnvs}
  normalize_advantage: True
  gamma: 0.99
  tau: 0.95
  learning_rate: 1e-4
  kl_threshold: 0.0016
  # PPO batch collection
  horizon_length: 64 #32
  minibatch_size: 512 #512 #160 #32768
  mini_epochs: 8
  # PPO loss setting
  clip_value: True
  critic_coef: 2
  entropy_coef: 0.0
  e_clip: 0.2
  bounds_loss_coef: 0.0
  # grad clipping
  truncate_grads: False
  grad_norm: 1.0
  # snapshot setting
  save_best_after: 0
  save_frequency: 10 #100
  max_agent_steps: 5000000
  # tactile setting
  tactile_info: False
  tactile_seq_length: 5
  tactile_info_embed_dim: 64
  # ncf setting
  ncf_info: False
  ncf_use_gt: False
  ncf_output_dim: 2000
  ncf_pc_subsample: 1.0
  ncf_proprio_adapt: False


# padapt:
  

# params:
#   seed: ${...seed}
#   algo:
#     name: a2c_continuous

#   model:
#     name: continuous_a2c_logstd

#   network:
#     name: actor_critic
#     separate: False

#     space:
#       continuous:
#         mu_activation: None
#         sigma_activation: None
#         mu_init:
#           name: default
#         sigma_init:
#           name: const_initializer
#           val: 0
#         fixed_sigma: True
#     mlp:
#       units: [256, 128, 64]
#       activation: elu
#       d2rl: False

#       initializer:
#         name: default
#       regularizer:
#         name: None

#   load_checkpoint: ${if:${...checkpoint},True,False}
#   load_path: ${...checkpoint}

#   config:
#     name: ${resolve_default:FactoryTaskNutBoltPlace,${....experiment}}
#     full_experiment_name: ${.name}
#     env_name: rlgpu
#     multi_gpu: ${....multi_gpu}
#     ppo: True
#     mixed_precision: True
#     normalize_input: True
#     normalize_value: True
#     value_bootstrap: True
#     num_actors: ${....task.env.numEnvs}
#     reward_shaper:
#       scale_value: 1.0
#     normalize_advantage: True
#     gamma: 0.99
#     tau: 0.95
#     learning_rate: 1e-4
#     lr_schedule: fixed
#     schedule_type: standard
#     kl_threshold: 0.016
#     score_to_win: 20000
#     max_epochs: ${resolve_default:2048,${....max_iterations}}
#     save_best_after: 50
#     save_frequency: 100
#     print_stats: True
#     grad_norm: 1.0
#     entropy_coef: 0.0
#     truncate_grads: False
#     e_clip: 0.2
#     horizon_length: 32
#     minibatch_size: 2 #51
#     mini_epochs: 8
#     critic_coef: 2
#     clip_value: True
#     seq_len: 4
#     bounds_loss_coef: 0.0001
